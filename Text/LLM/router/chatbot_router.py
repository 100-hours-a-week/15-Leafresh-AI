# chatbot_router.py
from ..model.chatbot.LLM_chatbot_base_info_model import base_prompt, get_llm_response as get_base_info_llm_response, base_parser
from ..model.chatbot.LLM_chatbot_free_text_model import process_chat, clear_conversation, conversation_states, custom_prompt, get_llm_response as get_free_text_llm_response, retriever
from ..model.chatbot.chatbot_constants import label_mapping, ENV_KEYWORDS, BAD_WORDS, category_keywords
from fastapi import APIRouter, Query, HTTPException
from fastapi.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
from typing import Optional, Dict, Any, Generator
import json
import re
import logging
from fastapi.responses import StreamingResponse
from typing import Generator, AsyncGenerator
import asyncio
from urllib.parse import unquote  # URL ë””ì½”ë”©ì„ ìœ„í•œ import ì¶”ê°€

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

router = APIRouter()

# ë¹„-RAG ë°©ì‹ ì±Œë¦°ì§€ ì¶”ì²œ (SSE)
@router.get("/ai/chatbot/recommendation/base-info")
async def select_category(
    sessionId: Optional[str] = Query(None),
    location: Optional[str] = Query(None),
    workType: Optional[str] = Query(None),
    category: Optional[str] = Query(None)
):
    """
    ì‚¬ìš©ìì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹œí™˜ê²½ ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•˜ëŠ” SSE ì—”ë“œí¬ì¸íŠ¸
    """
    # URL ë””ì½”ë”©
    if location:
        location = unquote(location)
    if workType:
        workType = unquote(workType)
    if category:
        category = unquote(category)
    
    # ì…ë ¥ê°’ ê²€ì¦
    if not location:
        print("location ëˆ„ë½")
        raise HTTPException(
            status_code=400,
            detail={
                "status": 400,
                "message": "locationì€ í•„ìˆ˜ì…ë‹ˆë‹¤.",
                "data": None
            }
        )
    if not workType:
        print("workType ëˆ„ë½")
        raise HTTPException(
            status_code=400,
            detail={
                "status": 400,
                "message": "workTypeì€ í•„ìˆ˜ì…ë‹ˆë‹¤.",
                "data": None
            }
        )
    if not category:
        print("category ëˆ„ë½")
        raise HTTPException(
            status_code=400,
            detail={
                "status": 400,
                "message": "categoryëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.",
                "data": None
            }
        )

    # ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ ê²€ì¦
    if category not in label_mapping:
        print(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬: {category}")
        print(f"ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡: {list(label_mapping.keys())}")
        raise HTTPException(
            status_code=400,
            detail={
                "status": 400,
                "message": "ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒ í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "data": {
                    "invalidFields": ["category"]
                }
            }
        )

    # LLM í˜¸ì¶œì„ ìœ„í•œ prompt êµ¬ì„± (ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸)
    prompt = base_prompt.format(
        location=location,
        workType=workType,
        category=category
    )

    # SSE ì‘ë‹µ ìƒì„±
    def event_generator():
        try:
            # ì„¸ì…˜ ì •ë³´ë§Œ ì €ì¥ (RAG ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            if sessionId and sessionId not in conversation_states:
                conversation_states[sessionId] = {
                    "messages": [],
                    "category": category,
                    "base_category": category
                }

            # ì „ì²´ ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
            eng_label = label_mapping[category]

            for data_payload in get_base_info_llm_response(prompt, category=category):
                try:
                    event_type = data_payload.get("event")
                    data_from_llm_model = data_payload.get("data")

                    if not event_type or not data_from_llm_model:
                        continue  # ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°

                    if event_type == "challenge":
                        yield {
                            "event": "challenge",
                            "data": data_from_llm_model
                        }

                    elif event_type == "close":
                        try:
                            parsed_json_data = json.loads(data_from_llm_model) if isinstance(data_from_llm_model, str) else data_from_llm_model

                            if not isinstance(parsed_json_data, dict):
                                raise ValueError("íŒŒì‹±ëœ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")

                            if "data" not in parsed_json_data or not isinstance(parsed_json_data["data"], dict):
                                raise ValueError("íŒŒì‹±ëœ ë°ì´í„°ì— ìœ íš¨í•œ 'data' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

                            final_data = parsed_json_data["data"]

                            if "challenges" not in final_data:
                                raise ValueError("íŒŒì‹±ëœ ë°ì´í„°ì— 'challenges' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

                            yield {
                                "event": "close",
                                "data": json.dumps({
                                    "status": 200,
                                    "message": "ëª¨ë“  ì±Œë¦°ì§€ ì¶”ì²œ ì™„ë£Œ",
                                    "data": final_data
                                }, ensure_ascii=False)
                            }
                            return
                            
                        except Exception as e:
                            yield {
                                "event": "error",
                                "data": json.dumps({
                                    "status": 500,
                                    "message": f"íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                                    "data": None
                                }, ensure_ascii=False)
                            }

                    elif event_type == "error":
                        yield {
                            "event": "error",
                            "data": data_from_llm_model
                        }

                except Exception as e:
                    logger.error(f"ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
                    yield {
                        "event": "error",
                        "data": json.dumps({
                            "status": 500,
                            "message": f"ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                            "data": None
                        }, ensure_ascii=False)
                    }

        except Exception as e:
            logger.error(f"SSE ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
            yield {
                "event": "error",
                "data": json.dumps({
                    "status": 500,
                    "message": f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ë¡œ ì¶”ì²œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "data": None
                }, ensure_ascii=False)
            }
        finally:
            # ì—°ê²° ì¢…ë£Œ ì´ë²¤íŠ¸ëŠ” ì´ë¯¸ íŒŒì‹±ëœ ë°ì´í„°ì™€ í•¨ê»˜ ì „ì†¡ë˜ì—ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì „ì†¡í•˜ì§€ ì•ŠìŒ
            pass

    return EventSourceResponse(
        event_generator(),
        media_type="text/event-stream",
        ping= None 
    )

# LangChain ê¸°ë°˜ RAG ì¶”ì²œ
@router.get("/ai/chatbot/recommendation/free-text")
async def freetext_rag(
    sessionId: Optional[str] = Query(None),
    message: Optional[str] = Query(None)
):
    """
    ììœ  ì±„íŒ… ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹œí™˜ê²½ ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•˜ëŠ” SSE ì—”ë“œí¬ì¸íŠ¸ (ì§ì ‘ generate + streamer)
    """
    # URL ë””ì½”ë”© ì¶”ê°€
    if message:
        message = unquote(message)
    # ì…ë ¥ê°’ ê²€ì¦
    if not message or not message.strip():
        return EventSourceResponse(
            event_generator_error("messageëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.", 400),
            media_type="text/event-stream"
        )
    if len(message.strip()) < 5:
        return EventSourceResponse(
            event_generator_error("messageëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•˜ë©°, ìµœì†Œ 5ì ì´ìƒì˜ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.", 422),
            media_type="text/event-stream"
        )
    message_lower = message.lower()
    # ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ìš”ì²­ ì²´í¬
    category_reset_keywords = ["ì›ë˜", "ì²˜ìŒ", "ì´ì „", "ì›ë˜ëŒ€ë¡œ", "ê¸°ì¡´", "ì¹´í…Œê³ ë¦¬"]
    is_category_request = any(keyword in message_lower for keyword in category_reset_keywords)
    # í™˜ê²½ ê´€ë ¨ ìš”ì²­ì´ ì•„ë‹ˆê³ , ì¹´í…Œê³ ë¦¬ ìš”ì²­ë„ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê¸°ë³¸ ì‘ë‹µ (fallback ë¡œì§)
    is_env_related = any(k in message for k in ENV_KEYWORDS)
    contains_bad_words = any(b in message_lower for b in BAD_WORDS)
    async def event_generator():
        # ì„¸ì…˜ ì´ˆê¸°í™” (free-textëŠ” ëŒ€í™” ê¸°ë¡ì´ ì¤‘ìš”)
        if sessionId not in conversation_states:
            conversation_states[sessionId] = {
                "messages": [],
                "category": "ì œë¡œì›¨ì´ìŠ¤íŠ¸", # ê¸°ë³¸ê°’
                "base_category": "ì œë¡œì›¨ì´ìŠ¤íŠ¸"
            }
        # fallback ì¡°ê±´ ê²€ì‚¬
        if not is_category_request and (not is_env_related or contains_bad_words):
            logger.info(f"Fallback triggered: {message}")
            yield {
                "event": "error",
                "data": json.dumps({
                    "status": 200,
                    "message": "ì €ëŠ” ì¹œí™˜ê²½ ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•´ë“œë¦¬ëŠ” Leafresh ì±—ë´‡ì´ì—ìš”! í™˜ê²½ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë” ì˜ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.",
                    "data": None
                }, ensure_ascii=False)
            }
            return # fallback ë©”ì‹œì§€ ì „ì†¡ í›„ ì¢…ë£Œ

        # 1. ì¹´í…Œê³ ë¦¬ ë³€ê²½ ë¡œì§ (í‚¤ì›Œë“œ ê¸°ë°˜)
        print(f"ğŸš€FREE-TEXT ROUTER START ğŸš€")
        print(f"User message: {message}")
        print(f"Session ID: {sessionId}")
        
        current_category = conversation_states[sessionId].get("category", "ì œë¡œì›¨ì´ìŠ¤íŠ¸")
        message_lower = message.lower()
        
        # ì¹´í…Œê³ ë¦¬ ë³€ê²½ ê²€ì‚¬
        category_changed = False
        print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘: '{message_lower}'")
        print(f"ğŸ”¥ category_keywords íƒ€ì…: {type(category_keywords)}")
        if isinstance(category_keywords, dict):
            for category, keywords in category_keywords.items():
                print(f"   - ì¹´í…Œê³ ë¦¬ '{category}' í‚¤ì›Œë“œ í™•ì¸: {keywords}")
                if any(keyword in message_lower for keyword in keywords):
                    print(f"ğŸ¯ í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ! '{category}' ì¹´í…Œê³ ë¦¬ë¡œ ë³€ê²½")
                    conversation_states[sessionId]["category"] = category
                    current_category = category
                    category_changed = True
                    break
        else:
            print(f"category_keywordsê°€ dictê°€ ì•„ë‹˜: {category_keywords}")
        
        if not category_changed:
            print(f"âŒ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨. ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ '{current_category}' ìœ ì§€")
        
        # 2. context ì¶”ì¶œ (RAG)
        messages_history = "\n".join(conversation_states[sessionId]["messages"])
        
        # RAG ê²€ìƒ‰ ìˆ˜í–‰
        docs = retriever.get_relevant_documents(message)
        context = "\n".join([doc.page_content for doc in docs])
        logger.info(f"RAG ê²€ìƒ‰ ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(docs)}")
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}")

        prompt = custom_prompt.format(
            context=context,  # RAG í™œì„±í™”
            query=message,
            messages=messages_history,
            category=current_category
        )

        # LLM ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        try:
            full_response_text = ""
            for data_payload in get_free_text_llm_response(prompt, current_category):
                event_type = data_payload.get("event")
                data_from_llm_model = data_payload.get("data")

                if not event_type or not data_from_llm_model:
                    continue  # ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°

                # data_from_llm_modelì´ JSON ë¬¸ìì—´ì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹± (í‚¤ ì ‘ê·¼ ì „)
                parsed_data_payload = json.loads(data_from_llm_model) if isinstance(data_from_llm_model, str) else data_from_llm_model

                if event_type == "challenge":
                    # í† í° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ full_response_textì— ëˆ„ì 
                    token_text = parsed_data_payload.get("data", "")
                    full_response_text += token_text

                    # í´ë¼ì´ì–¸íŠ¸ì— í† í° ì „ì†¡
                    yield {
                        "event": "challenge",
                        "data": json.dumps({
                            "status": 200,
                            "message": "í† í° ìƒì„±",
                            "data": token_text
                        }, ensure_ascii=False)
                    }

                elif event_type == "close":
                    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (LLM ìµœì¢… ì‘ë‹µ ì „ì²´ë¥¼ ì €ì¥)
                    if sessionId in conversation_states:
                        conversation_states[sessionId]["messages"].append(f"AI: {full_response_text}")

                    # ìµœì¢… ì‘ë‹µ ë°ì´í„° ì „ì†¡
                    yield {
                        "event": "close",
                        "data": json.dumps({
                            "status": 200,
                            "message": "ëª¨ë“  ì±Œë¦°ì§€ ì¶”ì²œ ì™„ë£Œ",
                            "data": parsed_data_payload.get("data", None)
                        }, ensure_ascii=False)
                    }
                    return

                elif event_type == "error":
                    yield {
                        "event": "error",
                        "data": data_from_llm_model
                    }

        except Exception as e:
            yield {
                "event": "error",
                "data": json.dumps({
                    "status": 500,
                    "message": f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ë¡œ ì¶”ì²œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "data": None
                }, ensure_ascii=False)
            }
            return

    return EventSourceResponse(
        event_generator(),
        media_type="text/event-stream",
        ping= None 
    )

async def event_generator_error(message: str, status_code: int):
    """ì—ëŸ¬ ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„°"""
    logger.error(f"SSE error event: {message} (status: {status_code})")
    yield {
        "event": "error",
        "data": json.dumps({
            "status": status_code,
            "message": message,
            "data": None
        }, ensure_ascii=False)
    }

# ëª…ë ¹ì–´ ì˜ˆì‹œ (vllm ëª¨ë¸ ì‹¤í–‰)
# python3 -m vllm.entrypoints.openai.api_server \
#   --model /home/ubuntu/mistral/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db \
#   --host 0.0.0.0 \
#   --port 8800