from typing import List, Dict, Any, AsyncIterator
from datetime import datetime, timedelta
from dotenv import load_dotenv
import os
import traceback
# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import json
import logging
from huggingface_hub import login
import gc
# from Text.LLM.model.chatbot.shared_model import shared_model
import requests

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

VLLM_API_URL = os.getenv("VLLM_API_URL", "http://localhost:8800/v1/chat/completions")

class VLLMFeedbackClient:
    def __init__(self):
        self.api_url = VLLM_API_URL

    def get_feedback(self, messages, temperature=0.7, max_tokens=512, model=...):
        """
        vLLM(OpenAI í˜¸í™˜) ì„œë²„ì— ì±„íŒ… ë©”ì‹œì§€ë¡œ inference ìš”ì²­
        messages: OpenAI API í¬ë§·ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        """
        headers = {"Content-Type": "application/json"}
        payload = {
            "model": "/home/ubuntu/mistral_finetuned_v5/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/0d4b76e1efeb5eb6f6b5e757c79870472e04bd3a",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            data = response.json()
            # OpenAI í˜¸í™˜ ì‘ë‹µ: data["choices"][0]["message"]["content"]
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            raise RuntimeError(f"vLLM API ìš”ì²­ ì‹¤íŒ¨: {e}")

# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
shared_model = VLLMFeedbackClient() 

class FeedbackModel:
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(FeedbackModel, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            load_dotenv()
            
            # ì§€ì—° ë¡œë”©: ì‹¤ì œ ì‚¬ìš©í•  ë•Œë§Œ ëª¨ë¸ ë¡œë“œ
            self._model = None
            self._tokenizer = None
            
            # í•œê¸€ ê¸°ì¤€ìœ¼ë¡œ 5-6ë¬¸ì¥ì— ì ì ˆí•œ í† í° ìˆ˜ë¡œ ì¡°ì • (ì•½ 250-300ì)
            self.max_tokens = 2048
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
            self.prompt_template = os.getenv("FEEDBACK_PROMPT_TEMPLATE",
            """
            ë„ˆëŠ” í”¼ë“œë°± ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ì•„ë˜ì™€ ê°™ì€ JSON ì…ë ¥ì„ ë°›ìœ¼ë©´, ì‚¬ìš©ìì˜ ì±Œë¦°ì§€ í™œë™ì„ ìš”ì•½í•´ì„œ ì¹­ì°¬ê³¼ ê²©ë ¤ë¥¼ í•œê¸€ë¡œ í•´ì¤˜.
            1. ìœ ë‹ˆì½”ë“œ(Unicode) í‘œì¤€ì— í¬í•¨ëœ ì´ëª¨ì§€(ì˜ˆ: ğŸ˜Š, ğŸŒ±, ğŸ‰ ë“±)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼í•˜ê³  ë°ì€ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            2. ì„±ê³µí•œ ì±Œë¦°ì§€ì— ëŒ€í•´ì„œëŠ” ì‚¬ìš©ìì˜ ë…¸ë ¥ì„ ì¸ì •í•˜ê³  ê²©ë ¤í•˜ëŠ” ë©”ì„¸ì§€ë¥¼ í¬í•¨í•´ì£¼ê³ , ì‹¤íŒ¨í•œ ì±Œë¦°ì§€ì— ëŒ€í•´ì„œëŠ” ìœ„ë¡œì™€ í•¨ê»˜ ë‹¤ìŒ ê¸°íšŒë¥¼ ê¸°ëŒ€í•œë‹¤ëŠ” ë©”ì‹œì§€ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
            3. {personal_challenges}ì™€ {group_challenges} ê¸°ë¡ì„ í†µí•©í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            ê°œì¸ ì±Œë¦°ì§€:
            {personal_challenges}
            ë‹¨ì²´ ì±Œë¦°ì§€:
            {group_challenges}
            """)
            
            self._initialized = True

    @property
    def model(self):
        if self._model is None:
            # self._model = shared_model.model
            pass
        return self._model
    
    @property
    def tokenizer(self):
        if self._tokenizer is None:
            # self._tokenizer = shared_model.tokenizer
            pass
        return self._tokenizer

    def _is_within_last_week(self, date_str: str) -> bool:
        """ì£¼ì–´ì§„ ë‚ ì§œê°€ ìµœê·¼ ì¼ì£¼ì¼ ì´ë‚´ì¸ì§€ í™•ì¸"""
        try:
            date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            one_week_ago = datetime.now() - timedelta(days=7)
            return date >= one_week_ago
        except (ValueError, TypeError):
            return False

    def _format_personal_challenges(self, challenges: List[Dict[str, Any]]) -> str:
        if not challenges:
            return "ì°¸ì—¬í•œ ê°œì¸ ì±Œë¦°ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for challenge in challenges:
            status = "ì„±ê³µ" if challenge["isSuccess"] else "ì‹¤íŒ¨"
            formatted.append(f"- {challenge['title']} ({status})")
        return "\n".join(formatted)

    def _format_group_challenges(self, challenges: List[Dict[str, Any]]) -> str:
        if not challenges:
            return "ì°¸ì—¬í•œ ë‹¨ì²´ ì±Œë¦°ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for challenge in challenges:
            # ì‹¤ì²œ ê²°ê³¼ê°€ ìˆëŠ” ì±Œë¦°ì§€ë§Œ í•„í„°ë§
            submissions = challenge.get("submissions", [])
            if not submissions:
                continue

            # ìµœê·¼ ì¼ì£¼ì¼ ì´ë‚´ì˜ ì œì¶œë§Œ í•„í„°ë§
            recent_submissions = [
                s for s in submissions
                if self._is_within_last_week(s["submittedAt"])
            ]
            
            if not recent_submissions:
                continue

            success_count = sum(1 for s in recent_submissions if s["isSuccess"])
            total_count = len(recent_submissions)
            
            formatted.append(
                f"- {challenge['title']}\n"
                f"  ê¸°ê°„: {challenge['startDate']} ~ {challenge['endDate']}\n"
                f"  ìµœê·¼ ì¼ì£¼ì¼ ì„±ê³µë¥ : {success_count}/{total_count}"
            )
        return "\n".join(formatted) if formatted else "ìµœê·¼ ì¼ì£¼ì¼ ë™ì•ˆ ì°¸ì—¬í•œ ë‹¨ì²´ ì±Œë¦°ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."

    async def generate_feedback(self, data: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            """
            ì´ì „ ìš”ì²­ì˜ ë©”ëª¨ë¦¬ ì”ì—¬ë¬¼ ì •ë¦¬: ë‹¤ë¥¸ ìš”ì²­ì—ì„œ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ
            ê¹¨ë—í•œ ìƒíƒœì—ì„œ ì‹œì‘: ìƒˆë¡œìš´ í”¼ë“œë°± ìƒì„± ì „ì— ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
            ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€: ì—°ì† ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë¬¸ì œ í•´ê²°
            """
            # shared_model.cleanup_memory()
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if data.get("memberId") is None:  # 0ë„ ìœ íš¨í•œ ê°’ìœ¼ë¡œ ì²˜ë¦¬
                return {
                    "status": 400,
                    "message": "memberIdëŠ” í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤.",
                    "data": None
                }
            
            if not data.get("personalChallenges") and not data.get("groupChallenges"):
                return {
                    "status": 400,
                    "message": "ìµœì†Œ 1ê°œì˜ ì±Œë¦°ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "data": None
                }

            # ì±Œë¦°ì§€ ë°ì´í„° í¬ë§·íŒ…
            personal_challenges = self._format_personal_challenges(data.get("personalChallenges", []))
            group_challenges = self._format_group_challenges(data.get("groupChallenges", []))

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_template.format(
                personal_challenges=personal_challenges,
                group_challenges=group_challenges
            )

            messages = [
                {"role": "system", "content": "ë„ˆëŠ” í”¼ë“œë°± ì–´ì‹œìŠ¤í„´íŠ¸ì•¼."},
                {"role": "user", "content": prompt}
            ]

            try:
                # vLLM(OpenAI í˜¸í™˜) API í˜¸ì¶œ
                full_feedback = shared_model.get_feedback(
                    messages,
                    temperature=0.7,
                    max_tokens=self.max_tokens
                )

                if not full_feedback.strip():
                    return {
                        "status": 500,
                        "message": "ì„œë²„ ì˜¤ë¥˜ë¡œ í”¼ë“œë°± ìƒì„±ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        "data": None
                    }

                return {
                    "status": 200,
                    "message": "í”¼ë“œë°± ê²°ê³¼ ìˆ˜ì‹  ì™„ë£Œ",
                    "data": {
                        "feedback": full_feedback.strip()
                    }
                }

            except Exception as model_error:
                error_trace = traceback.format_exc()
                logger.error(f"Model Error: {str(model_error)}\nTrace: {error_trace}")
                return {
                    "status": 500,
                    "message": "ì„œë²„ ì˜¤ë¥˜ë¡œ í”¼ë“œë°± ê²°ê³¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "data": None
                }

        except Exception as e:
            error_trace = traceback.format_exc()
            logger.error(f"General Error: {str(e)}\nTrace: {error_trace}")
            return {
                "status": 500,
                "message": "ì„œë²„ ì˜¤ë¥˜ë¡œ í”¼ë“œë°± ê²°ê³¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "data": None
            }
