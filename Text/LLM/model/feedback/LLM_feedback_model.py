from typing import List, Dict, Any, AsyncIterator
from datetime import datetime, timedelta
from dotenv import load_dotenv
import os
import traceback
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import json
import logging
from huggingface_hub import login
import gc
from Text.LLM.model.chatbot.shared_model import shared_model

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ê³µìœ  ëª¨ë¸ ì‚¬ìš©
model = shared_model.model
tokenizer = shared_model.tokenizer

logger.info("Using shared Mistral model for feedback")

class FeedbackModel:
    def __init__(self):
        load_dotenv()
        
        # ê³µìœ  ëª¨ë¸ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
        self.model = model
        self.tokenizer = tokenizer
        
        logger.info("Feedback model initialized with shared model")
        
        # í•œê¸€ ê¸°ì¤€ìœ¼ë¡œ 5-6ë¬¸ì¥ì— ì ì ˆí•œ í† í° ìˆ˜ë¡œ ì¡°ì • (ì•½ 250-300ì)
        self.max_tokens = 500
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        self.prompt_template = os.getenv("FEEDBACK_PROMPT_TEMPLATE",
        """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì±Œë¦°ì§€ ì´ë ¥ì„ íŒë‹¨í•˜ì—¬ í”¼ë“œë°±ì„ í•´ì£¼ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. 
        1. ë‹¤ìŒ {personal_challenges}ì™€ {group_challenges} ê¸°ë¡ì„ í†µí•©í•˜ì—¬ ìš”ì•½í•˜ê³ , ì‚¬ìš©ìì˜ ë…¸ë ¥ì„ ì¸ì •í•˜ê³  ê²©ë ¤í•˜ëŠ” í”¼ë“œë°±ì„ í•œê¸€ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.
        2. ì‹¤íŒ¨í•œ ì±Œë¦°ì§€ì— ëŒ€í•´ì„œëŠ” ìœ„ë¡œì™€ í•¨ê»˜ ë‹¤ìŒ ê¸°íšŒë¥¼ ê¸°ëŒ€í•œë‹¤ëŠ” ë©”ì‹œì§€ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
        3. ìœ ë‹ˆì½”ë“œ(Unicode) í‘œì¤€ì— í¬í•¨ëœ ì´ëª¨ì§€(ì˜ˆ: ğŸ˜Š, ğŸŒ±, ğŸ‰ ë“±)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼í•˜ê³  ë°ì€ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        4. ê°™ì€ ì˜ë¯¸ì˜ ë¬¸ì¥ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , êµ¬ì²´ì ì´ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
        5. ë¬¸ì¥ì´ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šê²Œ ì™„ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        6. ë¬´ì¡°ê±´ ì „ì²´ ë‹µë³€ì€ í•œê¸€ ê¸°ì¤€ 250ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

        ê°œì¸ ì±Œë¦°ì§€:
        {personal_challenges}

        ë‹¨ì²´ ì±Œë¦°ì§€:
        {group_challenges}
        """)

    def _is_within_last_week(self, date_str: str) -> bool:
        """ì£¼ì–´ì§„ ë‚ ì§œê°€ ìµœê·¼ ì¼ì£¼ì¼ ì´ë‚´ì¸ì§€ í™•ì¸"""
        try:
            date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            one_week_ago = datetime.now() - timedelta(days=7)
            return date >= one_week_ago
        except (ValueError, TypeError):
            return False

    def _format_personal_challenges(self, challenges: List[Dict[str, Any]]) -> str:
        if not challenges:
            return "ì°¸ì—¬í•œ ê°œì¸ ì±Œë¦°ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for challenge in challenges:
            status = "ì„±ê³µ" if challenge["isSuccess"] else "ì‹¤íŒ¨"
            formatted.append(f"- {challenge['title']} ({status})")
        return "\n".join(formatted)

    def _format_group_challenges(self, challenges: List[Dict[str, Any]]) -> str:
        if not challenges:
            return "ì°¸ì—¬í•œ ë‹¨ì²´ ì±Œë¦°ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for challenge in challenges:
            # ì‹¤ì²œ ê²°ê³¼ê°€ ìˆëŠ” ì±Œë¦°ì§€ë§Œ í•„í„°ë§
            submissions = challenge.get("submissions", [])
            if not submissions:
                continue

            # ìµœê·¼ ì¼ì£¼ì¼ ì´ë‚´ì˜ ì œì¶œë§Œ í•„í„°ë§
            recent_submissions = [
                s for s in submissions
                if self._is_within_last_week(s["submittedAt"])
            ]
            
            if not recent_submissions:
                continue

            success_count = sum(1 for s in recent_submissions if s["isSuccess"])
            total_count = len(recent_submissions)
            
            formatted.append(
                f"- {challenge['title']}\n"
                f"  ê¸°ê°„: {challenge['startDate']} ~ {challenge['endDate']}\n"
                f"  ìµœê·¼ ì¼ì£¼ì¼ ì„±ê³µë¥ : {success_count}/{total_count}"
            )
        return "\n".join(formatted) if formatted else "ìµœê·¼ ì¼ì£¼ì¼ ë™ì•ˆ ì°¸ì—¬í•œ ë‹¨ì²´ ì±Œë¦°ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."

    async def generate_feedback(self, data: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            """
            ì´ì „ ìš”ì²­ì˜ ë©”ëª¨ë¦¬ ì”ì—¬ë¬¼ ì •ë¦¬: ë‹¤ë¥¸ ìš”ì²­ì—ì„œ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ
            ê¹¨ë—í•œ ìƒíƒœì—ì„œ ì‹œì‘: ìƒˆë¡œìš´ í”¼ë“œë°± ìƒì„± ì „ì— ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
            ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€: ì—°ì† ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë¬¸ì œ í•´ê²°
            """
            shared_model.cleanup_memory()
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            if data.get("memberId") is None:  # 0ë„ ìœ íš¨í•œ ê°’ìœ¼ë¡œ ì²˜ë¦¬
                return {
                    "status": 400,
                    "message": "memberIdëŠ” í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤.",
                    "data": None
                }
            
            if not data.get("personalChallenges") and not data.get("groupChallenges"):
                return {
                    "status": 400,
                    "message": "ìµœì†Œ 1ê°œì˜ ì±Œë¦°ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "data": None
                }

            # ì±Œë¦°ì§€ ë°ì´í„° í¬ë§·íŒ…
            personal_challenges = self._format_personal_challenges(data.get("personalChallenges", []))
            group_challenges = self._format_group_challenges(data.get("groupChallenges", []))

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_template.format(
                personal_challenges=personal_challenges,
                group_challenges=group_challenges
            )

            try:
                # Mistral ëª¨ë¸ì„ í†µí•œ í”¼ë“œë°± ìƒì„±
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
                prompt_length = inputs["input_ids"].shape[1]  # í”„ë¡¬í”„íŠ¸ í† í° ê¸¸ì´
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.max_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                # ìƒì„±ëœ ì „ì²´ ì‹œí€€ìŠ¤ì—ì„œ í”„ë¡¬í”„íŠ¸ ì´í›„ ë¶€ë¶„ë§Œ ë””ì½”ë”©
                generated_ids = outputs[0][prompt_length:]
                full_feedback = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

                if not full_feedback.strip():
                    return {
                        "status": 500,
                        "message": "ì„œë²„ ì˜¤ë¥˜ë¡œ í”¼ë“œë°± ìƒì„±ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        "data": None
                    }

                callback_url = f"http://34.64.183.21:8080/api/members/feedback/result"
                return {
                    "status": 200,
                    "message": "í”¼ë“œë°± ê²°ê³¼ ìˆ˜ì‹  ì™„ë£Œ",
                    "data": {
                        "feedback": full_feedback.strip()
                    }
                }

            except Exception as model_error:
                error_trace = traceback.format_exc()
                logger.error(f"Model Error: {str(model_error)}\nTrace: {error_trace}")
                return {
                    "status": 500,
                    "message": "ì„œë²„ ì˜¤ë¥˜ë¡œ í”¼ë“œë°± ê²°ê³¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "data": None
                }

        except Exception as e:
            error_trace = traceback.format_exc()
            logger.error(f"General Error: {str(e)}\nTrace: {error_trace}")
            return {
                "status": 500,
                "message": "ì„œë²„ ì˜¤ë¥˜ë¡œ í”¼ë“œë°± ê²°ê³¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "data": None
            }
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            # ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ í•­ìƒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰
            shared_model.cleanup_memory()
            logger.info("Feedback model memory cleanup completed")
